{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/costadev00/AI-algorithms-from-scratch/blob/main/sentiment_analysis_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnnwupedFEIV"
      },
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "Let's use the sentiment analysis model you created to train a model using a dataset of Amazon product reviews, where we have the corresponding sentiment as a number between -1 and 1 (completely negative to completely positive).\n",
        "\n",
        "Note: Before running anything, go to Runtime -> Change Runtime Type -> T4 GPU to speed things up.\n",
        "\n",
        "First, we'll download the raw text file."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/gptandchill/sentiment-analysis\n",
        "%cd sentiment-analysis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQEeXqFNr8zN",
        "outputId": "80b80fed-4b60-48c1-e4bb-3305990ab784"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sentiment-analysis'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 6 (delta 1), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (6/6), 116.33 KiB | 768.00 KiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n",
            "/content/sentiment-analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the dataset, let's convert it into PyTorch tensors. You can just run and ignore the details of parsing the text file."
      ],
      "metadata": {
        "id": "YX0EIM55z_cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "  print(\"Using GPU\")\n",
        "else:\n",
        "  print(\"Using CPU\")\n",
        "list_of_strings = []\n",
        "list_of_labels = []\n",
        "\n",
        "import csv\n",
        "with open('EcoPreprocessed.csv') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    for row in reader:\n",
        "      list_of_strings.append(row[1])\n",
        "      list_of_labels.append(float(row[2]))\n"
      ],
      "metadata": {
        "id": "eTFmS9HLsRs9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82c198fa-b93b-4bd1-bcb3-9ed1ccca93dc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create the actual tensors, we can borrow the solution code from the \"NLP Intro\" problem."
      ],
      "metadata": {
        "id": "4bNwK6Yn1U-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(list_of_strings):\n",
        "\n",
        "    # First let's get the total set of words\n",
        "    words = set()\n",
        "    for sentence in list_of_strings:\n",
        "        for word in sentence.split():\n",
        "            words.add(word)\n",
        "\n",
        "    vocab_size = len(words)\n",
        "\n",
        "    # Now let's build a mapping\n",
        "    sorted_list = sorted(list(words))\n",
        "    word_to_int = {}\n",
        "    for i, c in enumerate(sorted_list):\n",
        "        word_to_int[c] = i + 1\n",
        "\n",
        "    # Write encode() which is used to build the dataset\n",
        "\n",
        "    def encode(sentence):\n",
        "        integers = []\n",
        "        for word in sentence.split():\n",
        "            integers.append(word_to_int[word])\n",
        "        return integers\n",
        "\n",
        "    var_len_tensors = []\n",
        "    for sentence in list_of_strings:\n",
        "        var_len_tensors.append(torch.tensor(encode(sentence)))\n",
        "\n",
        "    return vocab_size + 1, nn.utils.rnn.pad_sequence(var_len_tensors, batch_first = True), word_to_int"
      ],
      "metadata": {
        "id": "-Jv1RkGf1JNV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size, training_dataset, word_to_int = get_dataset(list_of_strings)\n",
        "training_labels = torch.unsqueeze(torch.tensor(list_of_labels), dim = -1)"
      ],
      "metadata": {
        "id": "P2d-Zrfk1tK1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the dataset is in, let's use the model you wrote. The only change we will make is replacing the Sigmoid layer with a Tanh layer. Tanh outputs are always between -1 and 1, so that makes more sense given the data labels."
      ],
      "metadata": {
        "id": "v73BawFa2Mrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionPredictor(nn.Module):\n",
        "    def __init__(self, vocabulary_size: int, embedding_dimension: int):\n",
        "        super().__init__()\n",
        "        self.embedding_layer = nn.Embedding(vocabulary_size, embedding_dimension)\n",
        "        self.linear_layer = nn.Linear(embedding_dimension, 1)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeddings = self.embedding_layer(x)\n",
        "        averaged = torch.mean(embeddings, axis = 1)\n",
        "        projected = self.linear_layer(averaged)\n",
        "        return self.tanh(projected)"
      ],
      "metadata": {
        "id": "OP0c76A32WPX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we train the model using our standard training loop. One difference you will notice is that I choose a random 64 sized subset of the total dataset (thousands and thousands of examples), which speeds up training."
      ],
      "metadata": {
        "id": "-u6u5V3I5ZyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dimension = 256\n",
        "model = EmotionPredictor(vocab_size, embedding_dimension)\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "for i in range(1000):\n",
        "  randperm = torch.randperm(len(training_dataset))\n",
        "  training_dataset, training_labels = training_dataset[randperm], training_labels[randperm]\n",
        "  mini_batch = training_dataset[:64]\n",
        "  mini_batch_labels = training_labels[:64]\n",
        "  pred = model(mini_batch)\n",
        "  optimizer.zero_grad()\n",
        "  loss = loss_function(pred, mini_batch_labels)\n",
        "  if i % 100 == 0:\n",
        "    print(loss.item())\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "id": "KDEStrPL2vxZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3545a8f3-d13f-4a7f-bab1-cd3117ccd1d2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.1764390468597412\n",
            "0.14054584503173828\n",
            "0.11578645557165146\n",
            "0.14513157308101654\n",
            "0.10159990936517715\n",
            "0.10930142551660538\n",
            "0.08291476964950562\n",
            "0.07910086214542389\n",
            "0.07745134830474854\n",
            "0.09713004529476166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see the model's outputs on some examples it's never seen before!\n",
        "\n"
      ],
      "metadata": {
        "id": "4H6OimX05pJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_one = \"worst movie ever\"\n",
        "\n",
        "example_two = \"best movie ever\"\n",
        "\n",
        "example_three = \"weird but funny movie\"\n",
        "\n",
        "examples = [example_one] + [example_two] + [example_three]\n",
        "\n",
        "# Let's encode these strings as numbers using the dictionary from earlier\n",
        "var_len = []\n",
        "for example in examples:\n",
        "  int_version = []\n",
        "  for word in example.split():\n",
        "    int_version.append(word_to_int[word])\n",
        "  var_len.append(torch.tensor(int_version))\n",
        "\n",
        "testing_tensor = torch.nn.utils.rnn.pad_sequence(var_len, batch_first=True)\n",
        "model.eval()\n",
        "\n",
        "print(model(testing_tensor).tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYW_lHXz5r8I",
        "outputId": "7bb9b70a-3453-496d-e1db-1d05c186afd7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.9989031553268433], [0.9993994235992432], [0.9810601472854614]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should find that the model outputs something close to -1 for example one, something very close to 1 for example two, and something close to 0 for example three (neutral)!\n",
        "\n",
        "This was a very simple model, and we will build more complex ones in the next problems!"
      ],
      "metadata": {
        "id": "sSCZ-Z2T9bO5"
      }
    }
  ]
}